1. 트리 기반 알고리즘(Random Forest, XGBoost)
- 모델에서 얻어지는 예측값은 학습데이터에 존재하는 관측값의 범위 안에만 있음
  1.1 Boosting vs Bagging

  - Boosting은 오분류된 관측치에 가중치를 더 주는 방식으로 sequential 하게 학습이 진행됨

  - 따라서 Boosting은 병렬처리가 어려움

    

2. 선형 알고리즘
  2.1 선형 회귀분석

  - 종속 변수 y와 한 개 이상의 독립 변수 (또는 설명 변수) X와의 선형 상관 관계를 모델링하는 회귀분석 기법이다. 
  - 한 개의 설명 변수에 기반한 경우에는 단순 선형 회귀, 둘 이상의 설명 변수에 기반한 경우에는 다중 선형 회귀라고 한다.
  - 단순/다중 선형 회귀분석 모두 종속변수 Y가 연속형일때 사용함
  - 회귀계수를 추정할 때 train 데이터의 오차제곱합을 최소화시키는 값을 찾는다.
  - 오차에 대한
    1) 독립성: 오차의 추정치인 잔차에 대한 Durbin-Watson 검정
    2) 등분산성: 독립변수와 잔차에 대한 산점도를 통해 시각적으로 파악
    3) 정규성: 오차의 정규성은 표준화된 잔차(삭제된 표준화잔차)에 대한 정규성 검정으로 확인
  - 다중공선성에 대한
    - 다중회귀분석에서 다중공선성은 통계의 가정과 관계없지만, 다중회귀 결과를 해석할 때 중요하다
    - 다중공선성은 분산팽창지수(VIF) 라는 통계량을 사용해 계산 가능하며 VIF값이 1에 가까울수록 다중공선성이 작고, 반대로 값이 커질수록 다중공선성의 정도도 커짐
      (보통 VIF가 10 이상인 경우 해당 변수가 다중공선성이 있다고 판단함)

  2.2 로지스틱 회귀분석

  - log odds와 독립변수 사이의 선형성을 가정하며 종속변수가 범주형일 때 사용하는 회귀분석이다.
  - 회귀계수를 추정할 때, likelihood를 최대화시키는 값을 찾음
  - mutinomial logistic regression 의 경우 y의 범주가 3개 이상이며 명목형일때 사용하는 로지스틱 회귀분석이다.

  2.3 proportional odds model

  - 일반화된 선형모델 중 하나로 이산 또는 연속 공변량에 대한 순서 반응의 의존성을 모델링하는 데 사용되는 모델이다.

  - 로지스틱 회귀분석과 마찬가지로 log odds와 독립변수 사이의 선형성을 가정한다.

  - proportional odds assumption을 확인한 후 사용해야 한다.
    (각 independent variable이 dependent variable에 대해 갖는 odds ratio가 dependent variable의 각 response variable에 대해 일정해야한다는 가설)

  - multinomial 로지스틱 회귀분석에 비해 예측이 필요한 회귀계수의 숫자가 작다.

  - 동일한 데이터에 두 모델을 적용하였을 때, mutinomial 로지스틱 회귀모델의 추정치들이 proportional odds model의 추정치에 비하여 더 큰 standard error를 갖는 경향이 있다.

    


3. Naïve Bayes Classification (나이브베이즈 분류)
- 특성들 사이의 독립을 가정하는 베이즈 정리를 적용한 확률 분류기의 일종이다

- 나이브 베이즈 모델의 파라미터 추정은 최대우도방법 (Maximum Likelihood Estimation (MLE))을 사용한다.

- 분류에 필요한 파라미터를 추정하기 위한 트레이닝 데이터의 양이 매우 적다

- 전반적인 신뢰수준과 틀린 예측을 해결하기 위해서 확률을 Laplace smoothin 과 같은 방법을 적용할 수 있다.
  (예) 문서에 등장하는 수만큼의 우도확률 곱으로 분류를 수행하기 때문에 단어 수가 늘어날 수록 그 값이 0으로 수렴하는 경향이 있다.
  긍정문서에 단 한번도 등장하지 않은 단어가 있다면 해당 단어의 우도는 0이 되기 때문에 분류과정에 문제 발생 -> smoothing!
  -> 나이브 베이즈 분류기에서 smoothing 기법은 오버피팅을 피하기 위하여 사용됩니다.

- 몇몇 변수들이 결측인 경우에도 분류문제에 활용할 수 있다. (다른 모델 neural network, decision tree는 결측치에 대한 사전 처리가 필요함)

  

4. SVM(support vector machine)

- 두 카테고리 중 어느 하나에 속한 데이터의 집합이 주어졌을 때, SVM 알고리즘은 주어진 데이터 집합을 바탕으로 하여 새로운 데이터가 어느 카테고리에 속할지 판단하는 비확률적 이진 선형 분류 모델을 만든다. 

- 만들어진 분류 모델은 데이터가 사상된 공간에서 경계로 표현되는데 SVM 알고리즘은 그 중 가장 큰 폭을 가진 경계를 찾는 알고리즘이다.

- non-support vector: SVM 분류모형에 영향을 주지 않는 vectors이다. SVM의 장점은 train 시 이러한 non-support vetor 를 제거하고 test 단계를 들어갈 수 있다는 것이다.

  

5. 통계

   5.1 신뢰구간, Bayesian Posterior interval

   - prior 분포: 사전 분포는 베이지안 추정 작업을 하기 전에 이미 알고 있던 모수  μ 의 분포를 뜻한다. 
     모수의 분포에 대해 아무런 지식이 없는 경우에는 균일(uniform) 분포  Beta(1,1) 나 0을 중심으로 가지는 정규분포  N(0,σ20)  등의 무정보분포(non-informative distribution)를 사용할 수 있다. 
   - 신뢰구간의 margin of error : sample이 모집단의 데이터 분포를 반영하는 정보를 얼마나 기대할 수 있는지를 나타냄
     margin of error = Z점수 * 모집단 표준편차 / 루트(표본 크기)

   5.2 모델의 편차와 분산은 tradeoff 관계

   - overfitting 된 모델: 편차는 감소시키나, 분산은 증가시킴
   - underfitting 된 모델: 편차는 증가시키나, 분산은 감소시킴

   5.3 검정에 대한 논의

   카이제곱 검정

   - 관찰된 빈도가 기대되는 빈도와 의미있게 다른지의 여부를 검증하기 위해 사용되는 검증방법으로 자료가 빈도로 주어졌을 때, 특히 명목척도 자료의 분석에 이용된다.
   - 1) 동질성 검증: '변인의 분포가 이항분포나 정규분포와 동일하다'라는 가설을 설정한다. 이는 어떤 모집단의 표본이 그 모집단을 대표하고 있는지를 검증하는 데 사용한다. 
   - 2) 독립성 검증: 변인이 두 개 이상일 때 사용되며, 기대빈도는 '두 변인이 서로 상관이 없고 독립적'이라고 기대하는 것을 의미하며 관찰빈도와의 차이를 통해 기대빈도의 진위여부를 밝힌다.

   

6. 분포에 대한 논의
   6.1. Negative binomial destribution

   - r번 성공할 때까지의 실패의 횟수를 x라고 하면, 확률변수(random variable), x는 음이항분포를 따릅니다. 
   - 이항분포와 같이 복원추출(with replacement)입니다.
   - 40%인 베르누이시행에서 2번 성공할 때 까지 1번 실패하는 경우를 음이항분포를 사용해 구해보면 다음과 같습니다.  negbinomdist(1,2,40%) = 19.2%
   - 포아송 모형에 적용할 때 발생하는 두 가지 공통된 문제, 즉 과도분산 및 0과잉을처리하기 위한 모형이다

   6.2 포아송 모델

   - 특정사건이 단위시간이나 공간 내에서 발생할 확률은 나머지 단위들에 대하여 독립적임을 가정하고 평균과 분산이 같다는 등분산(equi-dispersion)을 가정한다.
   - 포아송모형에서 가정하는 등분산은 분산이 평균을 초과하는 과도분산(over-dispersion)을 쉽게 관찰할 수 있다.
   - 과도분산은 자료안의 관찰되지 않은 상이성이나 자료안의 0의 과잉에 의해 초래된다 -> 모형측정의 효율성 감소
   - 관측치에 0이 많을 경우, 평균과 분산이 같다는 포아송 모형의 가정이 만족하지 않음 (관측치에 0이 많을 경우 포상송모델을 활용할 수 없음)

   6.3 Gaussian 모델 

   - 가우시안 분포는 보통 정규분포(standard distribution)로 알려져 있다.
   - 평균이 0, 분산이 1 분포 
   - 관측치에 0이 많을 경우 모델링에 좋지 않음(잘모르겠음)

   6.4 Binomial 모델 

   - 연속된 n번의 독립적 시행에서 각 시행이 확률 p를 가질 때의 이산 확률 분포이다. 

   - 이러한 시행은 베르누이 시행이라고 불리기도 한다. 사실, n=1일 때 이항 분포는 베르누이 분포이다.

   - 일반적인 주사위를 10회 던져서 숫자 6이 나오는 횟수를 센다. 이 분포는 n = 10이고 p = 1/6인 이항분포이다.

   - 관측치에 0이 많을 경우 모델링에 좋지 않음(잘모르겠음)

     

7. 딥러닝의 문제
   7.1 vanishing gradient 문제

   - 깊이가 깊은 네트워크를 사용하는 경우
   - weight initialization의 weight가 커지는 경우 activation output이 -1 또는 1로 saturate 되어 vanishing gradient 문제 발생 (반대도 아마 생길듯)

   7.2 overfitting(일반화가 안되는 경우)

   - batch size를 증기시키는 경우 sharp minima에 converge 하는 경우가 많고 이로인해 일반화 문제가 생김
   - Epoch를 증가시키는 경우
   - hidden layer의 수 및 hidden layer 내에서의 unit의 수를 증가시키는 경우

   7.3 overfitting을 해결하는 방법!

   - cost 함수에 regularization term 를 추가한다
   - 학습된 네트워크의 일부를 없애는 drop-out을 실행한다.
   - 일부 변수를 제외 후 재학습 한다.