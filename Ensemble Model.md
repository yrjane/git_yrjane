# Ensemble Model

## 1. 소개

### 1.1 정의

- "집단지성이 뛰어난 개인 한 사람보다 더 나은 결정을 내리는 경우가 많음"
- 훈련데이터 B개(보통은 B = 50, 100, 150, ...)마다 서로 다른 모형을 생성하여 결합한 뒤, 새로운 데이터를 예측할 때 통합된 모형을 사용하는 것. 주로 예측할 때 다수결 방식을 사용하는데 단순다수결 방식과 가중다수결 방식이 있음.
  - 단순다수결 : 배깅, 랜덤포레스트에서 사용 
  - 가중다수결 : 부스팅에서 사용 
- 좀 더 robust한, 예측모형/분류모형 생성 및 feature selection(중요변수선택)을 위해 사용함
- 분류모형, 예측모형에 모두 사용가능함. 근데 일반적으로 분류모형에 사용됨.

### 1.2. 앙상블모형의 성공요인 :: "Diversity"의 확보

- 앙상블 집합 내에 구축되는 모형(또는 분류기)  서로 유사하지 않고 매우 다양해야 함.

- 서로 다르고 다양한 B개의 분류기를 통합해야 데이터 자체에 존재하는 복잡/난해한 구조의 분류문제를 해결할 수 있다.

- 각각의 샘플 데이터마다 반드시 똑같은 방식의 모형을 쓸 필요는 없음 (예: B개의 Model 중 T_1=Decision Tree, T_2=Logistic Regression, ... , T_B=Neural Network 등등 사용가능)

  #### (1) 데이터의 다양성 :: Bootstrap

  - 하나의 훈련 데이터를 이용해서 서로 다른 데이터를 재생산해서 다양한 데이터를 만들어 내는 방법
  - Bootstrap 데이터 추출 방법 : 반복이 있는 확률임의 추출방법. 앙상블 방법 중 배깅과 랜덤포레스트에서 사용
  - 훈련데이터 전체가 L이라고 한다면, 부트스트랩 방법은 L_1,... , L_B개의 데이터를 생성

  #### (2) 모형생성의 다양성 :: 분할방법

  - 각각의 훈련데이터마다 다양한 모델을 적용하는 방법도 있으나, 보통은 Decision Tree를 사용하되, Model 적합시 분할방법에 변화를 주어 모형이 다양해지도록 함.
  - 랜포의 경우 중간 노드에서 분할후보점을 선발할 때 전체변수가 아니라 임의의 변수 부분집합 중에서 선발하고, 그 중에서 분할개선도를 최대화시키는 분할 point를 탐색함. (하나의 Decision Tree 모델 T_i에는 여러 개의 중간 노드가 있는데, 중간노드는 분할개선도를 최대화 시키는 분할점을 탐색하는 방식으로 만들어짐. 중간노드마다 후보변수의 부분집합이 랜덤하게 다르면, 동일한 방식으로 모델을 생성했다고 하더라도 생성될 때마다 다양성이 확보될 수 있음!)
  - 동일한 모형생성 방식을 사용하더라도 하이퍼파라미터 튜닝을 한다면 모형생성의 다양성을 확보하는 한가지 방법이 될 수 있음

## 2. 종류

### 2.1 Bagging :: Bootstrap Aggregating

[![https://i.ibb.co/xqY6Hv8/2020-06-06-10-36-11.jpg](https://github.com/Algoitthm/TCT/raw/master/202006_%EB%B6%84%EC%84%9D_%EC%A0%95%EB%A6%AC)](https://github.com/Algoitthm/TCT/blob/master/202006_분석_정리)

((안 뜨면 링크 타고 들어가서 확인))

![스크린샷 2020-06-06 오후 10.36.11](/Users/mellodian/Desktop/스크린샷 2020-06-06 오후 10.36.11.png)

- (https://www.youtube.com/watch?v=2Mg8QD0F1dQ)

- (1) 먼저 Train Data L을 뽑는다. (ex. 전체 데이터의 70%) ::  L={(x_i,y_i), i = 1, ..., n} 

  (2) L로부터 복원추출하여 부트스트랩 데이터를 B번 만든다 (일반적으로 B로 50을 많이 사용. )

  :: L_1, L_2, ... , L_B *** 데이터를 복원추출 시 B의 횟수를 아무리 늘려도 전체 데이터의 63.2%만 추출이 됨. 왜 63.2%인지. 자세한 내용은 검색을 통해 확인가능(keyword : bootstrap 0.632)

  (3) 각 부트스트랩 데이터마다 모델(분류기)를 생성 :: T_1, T_2, ... , T_B - 여기서 T는 Decision Tree

  (4) B개의 모델을 결합시켜 최종 앙상블모델을 생성. :: 범주형 모델(분류기)인 경우에는 다수결 방식에 따라 결정. :: 연속형 모델(Regression)인 경우에는 평균값을 사용

- 배깅방법은 데이터가 쪼금만 변해도 결과에 많은 변화가 생기는 불안정한 분류모델인 경우, 예측력을 획기적으로 향상시킨다고 알려져 있음. -> Decision Tree의 경우 pruning(가지치기)를 사용하지 않는 최대 Tree를 사용하는 것이 예측정확도가 좋다고 알려짐 (가지치기를 사용하지 않은 경우에 더 불안정한 모델이므로)

- Bagging 적용하면 안 되는 경우

  1. Too small : 표본데이터가 작은 경우 전체를 잘 반영하지 못함..
  2. Noise : 데이터에 noise가 많은 경우(ex. outlier가 크게 왜곡시킴)
  3. Dependency : 데이터의 독립성 가정

### 2.2 Boosting

(https://www.youtube.com/watch?v=GM3CDQfQ4sw)

- 배깅과 마찬가지로 B개의 분류기를 생성하여 종합하는 방법이지만, 분류기를 생성하는 방식과 종합하는 방식이 조금 다르다
- 무작위로 선택하는 것보다 약간 가능성이 높은 규칙(weak learner/classifier)들을 결합시켜 보다 정확한 예측 모델을 만들어 내는 것을 말함.
- 부스팅에 사용되는 분류기는 오분류율 기준으로 랜덤하게 예측하는 것보다 약간이라도 좋은 예측 Model 이기만 하면 효과가 있다고 알려짐. ==> 예측력이 약한 분류모델을 결합하여 강한 예측 모델을 만드는 과정!
- Adaboost 알고리즘( Adaptive Boosting)을 주로 사용함
- Boosting은 새로운 learner(classifier)를 학습할 때마다 바로 이전 결과를 참조하는 방식. 최종적으로 weak learner로부터의 출력을 결합하여 더 좋은 예측율을 갖는 strong learner를 만들어 냄.

_____ Boosting 사례) (이 부분은 '쉽게 읽는 머신 러닝(https://blog.naver.com/laonple)'에서 참고.)

- 예시) 가령 스팸 여부를 가릴 수 있는 방법을 개발한다고 해보자. 스팸 여부 판정할 수 있는 기준은 많으며, 다음과 같은 것들이 있다고 하자. (간단한 규칙들에 대하여 Yes/No로만 판정)

  - 링크만 있는 경우 => 스팸
  - 메일 내용에 “당신의 보험료가 xxx 입니다”라는 내용이 들어 있는 경우 => 스팸
  - 도메인 주소가 확실한 경우 => 스팸 아님
  - 보낸 사람이 확실한 경우 => 스팸 아님

  스팸 여부를 판단할 수 있는 기준들은 위에서 열거한 일부 경우뿐만 아니라 매우 많은 기준이 있을 수 있다. 그런데 위에서 열거한 기준들은 모든 메일에 똑같이 적용할 수 있을 정도로 확실한/강력한 규칙은 아니다. 이런 것들을 weak learner(rule, classifier)라고 한다. 무작위로 선정하는 것보다는 성공 확률이 높은, 다시 말하면 오차율이 50% 이하인 학습 규칙을 말한다

  Weak learner를 strong learner로 바꿀 수 있는 방법은 무엇일까? 간단한 방법은 다음과 같다.

  - 평균/ 가중 평균(weighted average)를 사용하는 방법
  - 가장 많은 의견(vote)을 얻은 것을 선정하는 방법

  앞서 살펴본 스팸 메일 여부를 가리는 규칙에 위 두가지 방법을 적용하면, 좀 더 강력한 스팸 필터가 만들어질 것이며, 규칙이 더 늘어나면 늘어날수록 더 좋은 결과를 기대할 수 있게 된다.

### 2.3 Bagging vs. Boosting

- Bagging은 모든 Bootstrap이 서로 독립적인 관계를 갖는다. 하지만 Boosting은 순차적으로 처리가 되며, 에러가 발생하면 그 에러의 weighting을 올리기 때문에 현재의 weak learner가 이전 weak learner의 영향을 받는다.
- Boosting은 최종적으로 weighted vote를 하지만, Bagging은 단순 vote를 한다.
- Bagging은 Variance를 줄이는 것이 주 목적이지만, Boosting은 Bias를 줄이는 것이 주 목적이다.
- 잡음이 없는 데이터에 대해서는 Boosting이 Bagging보다 우수하다.
- Bagging은 overfitting 문제를 해결할 수 있지만, Boosting은 overfitting 문제로부터 자유롭지 못하다