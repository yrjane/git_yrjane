1. 트리 기반 알고리즘(Random Forest, XGBoost)
- 모델에서 얻어지는 예측값은 학습데이터에 존재하는 관측값의 범위 안에만 있음
1.1 Boosting vs Bagging
- Boosting은 오분류된 관측치에 가중치를 더 주는 방식으로 sequential 하게 학습이 진행됨
- 따라서 Boosting은 병렬처리가 어려움

2. 선형 알고리즘
2.1 선형 회귀분석
- 종속 변수 y와 한 개 이상의 독립 변수 (또는 설명 변수) X와의 선형 상관 관계를 모델링하는 회귀분석 기법이다. 
- 한 개의 설명 변수에 기반한 경우에는 단순 선형 회귀, 둘 이상의 설명 변수에 기반한 경우에는 다중 선형 회귀라고 한다.
- 단순/다중 선형 회귀분석 모두 종속변수 Y가 연속형일때 사용함
- 회귀계수를 추정할 때 train 데이터의 오차제곱합을 최소화시키는 값을 찾는다.
* 오차에 대한
1) 독립성: 오차의 추정치인 잔차에 대한 Durbin-Watson 검정
2) 등분산성: 독립변수와 잔차에 대한 산점도를 통해 시각적으로 파악
3) 정규성: 오차의 정규성은 표준화된 잔차(삭제된 표준화잔차)에 대한 정규성 검정으로 확인

2.1 로지스틱 회귀분석
- log odds와 독립변수 사이의 선형성을 가정하며 종속변수가 범주형일 때 사용하는 회귀분석이다.
- 회귀계수를 추정할 때, likelihood를 최대화시키는 값을 찾음
- mutinomial logistic regression 의 경우 y의 범주가 3개 이상이며 명목형일때 사용하는 로지스틱 회귀분석이다.
2.2 proportional odds model
- 일반화된 선형모델 중 하나로 이산 또는 연속 공변량에 대한 순서 반응의 의존성을 모델링하는 데 사용되는 모델이다.
- 로지스틱 회귀분석과 마찬가지로 log odds와 독립변수 사이의 선형성을 가정한다.
- proportional odds assumption을 확인한 후 사용해야 한다.
 (각 independent variable이 dependent variable에 대해 갖는 odds ratio가 dependent variable의 각 response variable에 대해 일정해야한다는 가설)
- mutinomial 로지스틱 회귀분석에 비해 예측이 필요한 회귀계수의 숫자가 작다.
- 동일한 데이터에 두 모델을 적용하였을 때, mutinomial 로지스틱 회귀모델의 추정치들이 proportional odds model의 추정치에 비하여 더 큰 standard error를 갖는 경향이 있다.


3. Naïve Bayes Classification (나이브베이즈 분류)
- 특성들 사이의 독립을 가정하는 베이즈 정리를 적용한 확률 분류기의 일종이다
- 나이브 베이즈 모델의 파라미터 추정은 최대우도방법 (Maximum Likelihood Estimation (MLE))을 사용한다.
- 분류에 필요한 파라미터를 추정하기 위한 트레이닝 데이터의 양이 매우 적다
- 전반적인 신뢰수준과 틀린 예측을 해결하기 위해서 확률을 Laplace smoothin 과 같은 방법을 적용할 수 있다.
(예) 문서에 등장하는 수만큼의 우도확률 곱으로 분류를 수행하기 때문에 단어 수가 늘어날 수록 그 값이 0으로 수렴하는 경향이 있다.
긍정문서에 단 한번도 등장하지 않은 단어가 있다면 해당 단어의 우도는 0이 되기 때문에 분류과정에 문제 발생 -> smoothing!
-> 나이브 베이즈 분류기에서 smoothing 기법은 오버피팅을 피하기 위하여 사용됩니다.
- 몇몇 변수들이 결측인 경우에도 분류문제에 활용할 수 있다. (다른 모델 neural network, decision tree는 결측치에 대한 사전 처리가 필요함)


3. 신뢰구간, Bayesian Posterior interval
- prior 분포: 사전 분포는 베이지안 추정 작업을 하기 전에 이미 알고 있던 모수  μ 의 분포를 뜻한다. 
모수의 분포에 대해 아무런 지식이 없는 경우에는 균일(uniform) 분포  Beta(1,1) 나 0을 중심으로 가지는 정규분포  N(0,σ20)  등의 무정보분포(non-informative distribution)를 사용할 수 있다. 
- 신뢰구간의 margin of error : sample이 모집단의 데이터 분포를 반영하는 정보를 얼마나 기대할 수 있는지를 나타냄
margin of error = Z점수 * 모집단 표준편차 / 루트(표본 크기)

4. 모델의 편차와 분산은 tradeoff 관계
- overfitting 된 모델: 편차는 감소시키나, 분산은 증가시킴
- underfitting 된 모델: 편차는 증가시키나, 분산은 감소시킴

5. SVM(support vector machine)
- 두 카테고리 중 어느 하나에 속한 데이터의 집합이 주어졌을 때, SVM 알고리즘은 주어진 데이터 집합을 바탕으로 하여 새로운 데이터가 어느 카테고리에 속할지 판단하는 비확률적 이진 선형 분류 모델을 만든다. 
- 만들어진 분류 모델은 데이터가 사상된 공간에서 경계로 표현되는데 SVM 알고리즘은 그 중 가장 큰 폭을 가진 경계를 찾는 알고리즘이다.
- non-support vector: SVM 분류모형에 영향을 주지 않는 vectors이다. SVM의 장점은 train 시 이러한 non-support vetor 를 제거하고 test 단계를 들어갈 수 있다는 것이다.


6. 분포에 대한 논의
6.1. Negative binomial destribution
- r번 성공할 때까지의 실패의 횟수를 x라고 하면, 확률변수(random variable), x는 음이항분포를 따릅니다. 
- 이항분포와 같이 복원추출(with replacement)입니다.
- 40%인 베르누이시행에서 2번 성공할 때 까지 1번 실패하는 경우를 음이항분포를 사용해 구해보면 다음과 같습니다.  negbinomdist(1,2,40%) = 19.2%
- 포아송 모형에 적용할 때 발생하는 두 가지 공통된 문제, 즉 과도분산 및 0과잉을처리하기 위한 모형이다
6.2 포아송 모델
- 특정사건이 단위시간이나 공간 내에서 발생할 확률은 나머지 단위들에 대하여 독립적임을 가정하고 평균과 분산이 같다는 등분산(equi-dispersion)을 가정한다.
- 포아송모형에서 가정하는 등분산은 분산이 평균을 초과하는 과도분산(over-dispersion)을 쉽게 관찰할 수 있다.
- 과도분산은 자료안의 관찰되지 않은 상이성이나 자료안의 0의 과잉에 의해 초래된다 -> 모형측정의 효율성 감소
- 관측치에 0이 많을 경우, 평균과 분산이 같다는 포아송 모형의 가정이 만족하지 않음 (관측치에 0이 많을 경우 포상송모델을 활용할 수 없음)
6.3 Gaussian 모델 
- 가우시안 분포는 보통 정규분포(standard distribution)로 알려져 있다.
- 평균이 0, 분산이 1 분포 
- 관측치에 0이 많을 경우 모델링에 좋지 않음(잘모르겠음)
6.4 Binomial 모델 
- 연속된 n번의 독립적 시행에서 각 시행이 확률 p를 가질 때의 이산 확률 분포이다. 
- 이러한 시행은 베르누이 시행이라고 불리기도 한다. 사실, n=1일 때 이항 분포는 베르누이 분포이다.
- 일반적인 주사위를 10회 던져서 숫자 6이 나오는 횟수를 센다. 이 분포는 n = 10이고 p = 1/6인 이항분포이다.
- 관측치에 0이 많을 경우 모델링에 좋지 않음(잘모르겠음)


7. 검정에 대한 논의
7.1 카이제곱 검정
- 관찰된 빈도가 기대되는 빈도와 의미있게 다른지의 여부를 검증하기 위해 사용되는 검증방법으로 자료가 빈도로 주어졌을 때, 특히 명목척도 자료의 분석에 이용된다.
- 1) 동질성 검증: '변인의 분포가 이항분포나 정규분포와 동일하다'라는 가설을 설정한다. 이는 어떤 모집단의 표본이 그 모집단을 대표하고 있는지를 검증하는 데 사용한다. 
- 2) 독립성 검증: 변인이 두 개 이상일 때 사용되며, 기대빈도는 '두 변인이 서로 상관이 없고 독립적'이라고 기대하는 것을 의미하며 관찰빈도와의 차이를 통해 기대빈도의 진위여부를 밝힌다.ㅜㅑ
8. 딥러닝의 문제
8.1 vanishing gradient 문제
- 깊이가 깊은 네트워크를 사용하는 경우
- weight initialization의 weight가 커지는 경우 activation output이 -1 또는 1로 saturate 되어 vanishing gradient 문제 발생 (반대도 아마 생길듯)
8.2 overfitting(일반화가 안되는 경우)
- batch size를 증기시키는 경우 sharp minima에 converge 하는 경우가 많고 이로인해 일반화 문제가 생김
- Epoch를 증가시키는 경우
- hidden layer의 수 및 hidden layer 내에서의 unit의 수를 증가시키는 경우
8.3 overfitting을 해결하는 방법!
- cost 함수에 regularization term 를 추가한다
- 학습된 네트워크의 일부를 없애는 drop-out을 실행한다.
- 일부 변수를 제외 후 재학습 한다.




