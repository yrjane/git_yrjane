## 데이터 엔지니어링

(24p ~ 끝까지)

### ETL

- ETL은 Extract(추출) 하여 Transformation(변환) 하여 타겟시스템에 Load(적재) 하는 과정.
- 다양한 형식의 데이터를 한 시스템에 통일된 양식으로 모으는 작업.
- 데이터 이동과 관련된 모든 분야에 활용
  - BI 및 데이터 분석을 위한 수집
  - 마스터 데이터 관리
  - 데이터 Migration(e.g. 온프로미스에서 AWS로 서버 이전) + 빅데이터 이행과제 지원
  - 같은 데이터를 사용하는 두 시스템의 데이터 동기화
  - 기업 간 데이터 전송(e.g. 판토스의 물류이동 데이터를 화학에서 받아서 쓰기)
  - 데이터 서비스(e.g. 흩어져 있는 모든 데이터를 뽑을 수 있는 통합된 웹포탈)
- 갖추어야 할 것
  - 설계 개발이 되고, 프로그램을 실행한 것에 대한 모니터링이 가능
  - 다양한 시스템에 접근이 가능해야함.(AWS도 되고, 오라클도 되고, 엑셀 업로드도 되고...)
  - 보내는 것도 마찬가지임
- ETL 툴 선정 기준
  - 호환성(추가 변환작업없이 여러가지 포맷을 받고 여러가지 포맷으로 보낼 수 있는가)
  - 여러테이블을 join해서 하나로 만들거나 반대로 분할해주는 프로그램도 되는가?
  - GUI가 사용자 직관적인가?
  - 오류처리가 가능한가?
  - 보안이 유지되는가?
  - 한번에 여러개 돌릴 수 있는가?
  - 프로그램을 수정하면 연관된 프로그램에 얼만큼의 영향이 있는지 확인 가능한가?
  - 배치 스케줄링 할 수 있는가?
  - 소스 to 타겟 컬럼 자동매칭 (e.g. 소스와 타겟 테이블 컬럼이름이 같으면 일일히 입력안해도 자동으로 1초만에 연결되도록)
  - 작업 소요시간을 미리 알 수 있는지?

### 매핑 작업의 순서

1. 소스 시스템 분석
   1. 소스 시스템 내 테이블 분석
   2. 1에서 분석한 테이블의 컬럼 분석
2. 타겟 시스템 분석
   1. 데이터 넣을 DB구현
   2. 소스 시스템 분석을 토대로 타겟의 테이블, 컬럼 정의
3. 매핑 정의
4. 어떤 테이블의 어떤 컬럼과 연결시킬지 결정
5. 추출하는 횟수 설계
6. 데이터를 변경시키는 로직도 설계하기
7. 이 모든것을 매핑 정의서로 작성

### 추출 및 전송

1. 추출 전송 방식

   1. 온라인 : API나 쿼리문으로 받아오기
      - 일반적인 방식
      - 하지만 운영서버가 해당 데이터를 보내주기위한 작업을 너무 오래하면 안됨
      - 그래서 소량 전송할 때 주로 사용함
   2. 오프라인 : 파일로 받아오기
      - 귀찮음
      - 운영서버에 무리가 적어서 초기적재할 때 사용.

   - 그냥 온라인으로 초기적재 하기도함(운영서버 사용 잘 안하는 새벽에)

2. (운영 중) 추출 방식

   1. Refresh : 싹다 갈아엎기

   - 데이터 적은 것만, 디멘젼 같은 것만.

   1. Timestamp : 시간 비교
      - 데이터 변경일자를 확인하여 마지막 추출 일자보다 나중 것만 추출
      - 원래 소스시스템에 변경일자 컬럼이 있어야함
   2. Snapshot비교 : 데이터 적재된 것을 비교
      - 담겨있는 전체데이터를 '캡쳐'하여 추출 전 후를 비교
   3. DBMS 로그 : 변경 내역 관리
      - 로그에 변경내역만 올려둠
      - 로그를 확인하여 변경내역만 추출함
   4. Application log : 변경 내역 관리

   - 로그 파일말고 어플리케이션에 기록함
   - 파일 저장이아니고 어플리케이션에 또 무언가를 보내야하므로 부하가 크고 복잡

### 변환

- 데이터 표기 방식 통일
  - Y/N, 0/1, T/F를 로직을 통해 하나로 변경
  - 시간 표기 방식 통일
  - NULL값을 처리하는 방식
  - 불필요하게 다양한 Key값을 하나의 Key값으로 통일
  - 컬럼에 comment달기
  - 필요하다면 Min, Avg, Sum, Count 등의 집계함수 사용
  - 계산 및 로직이 필요한 컬럼(파생변수)

### 적재

- Append : 기존 테이블에 데이터 추가하기
- Delete + Append : e.g. 최근한달 데이터를 update할 경우 한달치를 delete한 뒤에 append
- Truncate : 기존의 데이터를 싹다 날리기
- Insert : append를 DB에서 하는 방식임
- Create : 임시 테이블을 생성하면서 적재함 (툴 내에서 자동으로 이뤄지는 case임.)
- 적재할 때는 적재 시간과 용량을 고려해야한다.(Tuning과 복잡한 key구조 제거,index 활용 등)

### 검증

1. data를 못불러와서 공백이나 NULL이 들어갔는지
2. 최소 최댓값 (창고의 재고가 마이너스면 이상하잖아?)
3. 건수 검증(1:1로 불러왔는데 왜 데이터 수가 달라?)
4. 코드데이터가 제대로 들어갔는지? (고객코드는 (0001) 4자리인데 왜 1자리밖에 없어)
5. 날짜 데이터 검증 (YYYYMMDD와 YYYY/MM/DD는 엄연히 다르다.)
6. 업무 로직 검증(코드번호가 S로 시작하면 샘플로 만든 제품이다... 등등)

### 배치

1. 고려해야할 사항
   1. 소스 시스템은 언제 배치되는지?(새 데이터는 5시에 만들어지는데 4시에 가져와봤자)
   2. 데이터 간 선후행 관계 고려(분석용 마트가 디멘젼보다 먼저 적재되면 안된다.)
   3. 언제 작업할 수 있는지?
   4. 동시에 몇개가 돌아갈 수 있는지?
   5. 각 batch를 그룹핑할 수 있는지?
2. 담당자가 처음부터 끝까지 보고있지 않아도 될정도로 되어야 함
   1. 오류가 발생하면 영향이있는 후속작업이 자동으로 멈추도록 해야함
   2. 후속작업은 선행작업 바로 다음에 연결하거나, 끝나는 시간을 예측하여 진행하도록함

### 정제

- 정제는 되도록 소스 데이터를 고칠 수 있도록 한다.
- 변환을 써서 고치면 당장은 맞겠지만, 로직이 복잡해지고 오랜시간 후에 소스가 고쳐졌을때 문제가 발생
- 바꿀 수 없다면 ETL에서 진행한다.
- 품질 관리 프로세스
  1. 데이터에 문제가 없는가?
  2. 약간 있는 문제가 큰 영향이 있는가?
  3. 이 데이터 담당부서는 어디고, 필요없는 부분은 있고, 무엇에 쓰이며, 바꾸는 비용은 얼마나 되는가?
  4. 바꿀 방법을 모색한다.
  5. 바꾸고 난 뒤 영향도에 따라 후속작업을 진행한다.
  6. 다시 품질확인을 하여 문제가 없는지 확인한다.
- (정치적인 방식)
  - 운영계 시스템에서 데이터에 대해 품질을 검증하고 개선할 수 있도록 해야한다.
  - 데이터가 이상함을 현업에게 고의로 노출하여 문제가 있음을 보여준다.
  - 최대한 소스쪽 수정을 요구하고, 정말 부득이한 경우에만 ETL에서 수정
