## 알고리즘 이론정리

### 회귀분석

회귀분석에서, 회귀계수값을 제대로 추정하고 이를 신뢰하기 위해서는 필요한 몇가지의 가정들이 있음.

1. 모델의 형태에 대한 가정

   Y는 X와 파라미터들의 선형 결합으로 이루어진 형태이다.

2. 에러(잔차)에 대한 가정

   - 에러 e1,e2..en이 iid(identically and independently) 하게 평균이 0이고 분산이 sigma^2인 정규분포를 따름

     -> 에러의 정규성: 에러는 정규분포를 따라야 함

     -> 에러의 등분산성: 에러는 constant 하여야 함 (증가하거나 감소하는 trend 가 존재해서는 안됨)

     -> 에러의 자기상관 부재: 에러들끼리는 서로 독립이어야 함

     -> 이때 해당 가정에서 정규성 가정을 만족하지 않아도 회귀계수 추정값은 unbiased estimator임.. 이때 비교대상을 선형이면서 Unbiased 추정량들의 집합으로 한정할 경우에, Least square estimator 는 불편추정량이자 최소 분산 (smallest variace) 을 지닌 최량선형불편추정량(Best Linear Unbiased Estimator)임 (가우스-마코브정리)

     **LSE(Least Square Estimator) -최제추정량이란?**

     - 회귀계수 추정방법으로, 최소제곱법을 이용하여 잔차의 제곱합이 최소가 되도록 함.

3. predictors 에 대한 가정

   - 설명변수들은(predictor variables) 모두 non-random이다

     -> 따라서 반응변수의 정규성은 에러의 정규성에서 기인함

   - 설명변수의 관측값 xij에는 error 가 존재하지 않는다

   - 설명변수 x1,x2,,,xp는 서로 독립이다.

다중공산성(Multicollinearity)

- 독립변수가 다른 독립변수들의 선형 결합으로 나타낼 수 있을때(즉, 독립변수들 사이에 서로 강한 연관관계가 존재할때) 발생하는 문제로, 이 경우 X tranpose x X 행렬의 역행렬이 존재하지 않는 문제가 발생하며 회귀계수 추정값을 LSE로 계산할수 없는 문제가 발생함.

  -> 다중공산성 문제가 발생할 경우, 개개 변수들이 반응변수에 미치는 Unique 한 영향도를 측정하는것이 불가능해 지며, 예측된 회귀계수값이 데이터의 변화에 매우 민감하게 변하는 문제가 발생함. 즉 모델 일반화가 힘들어짐.

  즉 예측한 회귀계수의 sampling error 값이 매우 커지기 때문에 해석과 예측 두가지 경우가 모두 힘들어짐

- 다중공산성 문제는 모델링 에러는 아니고, 대체로는 데이터의 부족 문제로 인해 발생함

- 다중공산성은 Variance Inflation Factor, VIF 를 통해 파악 할 수 있으나 이가 모든 다중공산성을 표현해 주지는 않음

  -> VIF 는 1/1-R^2로 표현되며, 통상적으로 vif가 10 이상일때 변수에 다중공산성이 존재한다고 파악하나 10이라는 숫자에 통계이론적인 근거는 찾기 힘듬

추가적으로, 회귀분석의 계수에 대해 해석할때는 "when other factors are constant" 라는 가정이 필요함. 다른 변수가 고정적일때를 가정하지 않고는 해당 변수가 회귀계수만큼 반응변수에 영향을 미친다고 해석 할 수 없음

### Generalized Linear Model(GLM)

numeric 변수만을 반응변수로 하는 회귀분석의 일반화된 모형으로, link function 의 종류에 따라 다양한 모델이 존재함.

GLM 은 3가지 부분으로 구성되어 있는데, random component 인 반응변수 Y, systematic component 인 설명변수 X, 그리고 Y와 X사이의 관계를 규명하는 link function임. 근데 여기서 link function 이 되기 위한 조건이 두가지 존재하는데,바로 monotone(단조증가) 하고 differentiable(미분가능) 하다는 점임. 이 두가지 조건을 만족하지 않은 함수는 link function 이 될 수 없음.

1. Poisson regression

   반응변수가 count data 일때 주로 사용함, 반응변수의 평균과 분산이 모두 모두 m(모수) 로 일정하며 link function g(m) = log(m) 임

   실제로 데이터의 분산(m)이 평균값(m) 보다 크게 될 때 포아송 회귀를 사용하게 되면 Overdispersion 문제가 발생하게 되며, 이는 데이터들의 이산성으로 인해 쉽게 발생하는 문제임

   -> 이 경우에는 Y의 평균과 분산값이 다르지만 동일한 link function 을 사용하는 Negative Binomial 회귀를 사용하면 해결됨. NB 회귀는 Y가 NB(m,k) 를 따른다고 가정하며, 이때 분산 k는 m + Dm^2으로 표현됨. D가 0으로 수렴함에 따라 overdispersion 문제가 발생하지 않는다고도 볼 수 있음.

2. Logistic regression

   반응변수가 Binary 일때 사용하는 회귀로, link function 으로 sigmoid 를 사용함. logit(p) = log(p/1-p) = a+bx. p는 성공확률

   다른 변수들이 constant 할 때, x가 1단위 증가할때 마다 오즈가 exp(beta) 배 증가한다고 해석. 베타=0 이면 x가 변화해도 오즈 변화 없음.

3. Multicategory Logit Model

   반응변수가 binary 이상의 범주를 가지고 있는 변수일때 사용하며 nominal일 경우 baseline category logit을, ordinal 의 경우 cumulative logit 을 사용함

   - baseline category logit

     -> Y1,Y2..Yn이 iid 하게 multinomial(1,p1,p2..pJ) 을 따른다고 가정함. J는 카테고리의 갯수. 마지막 카테고리 J가 baseline이 된다고 할 때, baseline category logit 은 log(pj/pJ), j = 1,2,...J-1 로 표현 가능.

     즉, log(pj/pJ) = aj + Bjx 이며 J=2 일땐 로지스틱 회귀 모형과 동일해짐.

     베이스라인 카테고리 로짓 모형은 카테고리를 짝지어서 비교하는 형태로 총 J-1개의 식이 동시에 피팅되게 되며 각각의 피팅에 비교하여 SE가 줄어듬.

   - Cumulative logit

     -> 반응변수의 카테고리가 ordered 된 경우에 로짓에 순서 정보를 반영할수 있고, 베이스라인 로짓 모델에 비해 해석이 간단하며 성능이 좋음.

     logit(P(Y<=j)) = log(P(Y<=j)/1-P(Y<=j)) = log[(p1+p2..pj) / (p1+p2...pJ)] = aj + Bx

     베이스라인 모형과는 다르게 변수 x가 반응변수에 미치는 영향이 J-1개 모두 B로 동일함. 즉 추정되는 베타 계수가 한개인데 이를 적용하기 위해서는 각각의 cumulative 확률에 같은 확률 베타가 적용된다는 proportional odds 가정이 필요함(확률 오즈 모델과 j번째 카테[고리에 대한 분리 모형 Bj와 비교함)

GLM 모델들을 서로 비교할때는 Deviance 를 이용함.

H0: 현재모델 유지 H1: full model

Lm, Ls를 각각의 모델에 대한 maximized log-likelihood라고 할때, GLM의 deviance 는 -2(Lm-Ls) 로 표현 가능하며 이를 LR statistic 이라고 함. 해당 통계량은 자유도가 두 모델의 파라미터 갯수 차이인 카이제곱 분포에 근사하며, 값이 클수록 귀무가설을 기각.

### 나이브베이즈 분류기(Naive Bayesian Classifier)

스팸메일 필터, 텍스트 분류, 감정분석, 추천시스템 등에 활용되는 분류 기법으로 지도학습의 일종이며 feature에 따라 라벨을 분류함(반응변수를 분류함)에 있어서 베이즈 정리를 사용함. 또한, 모든 설명변수(feature)들이 서로 독립이라는 가정이 필요함. 특정 개체 x가 특정그룹 c에 속할 확률 즉 p(c|x) 이하 사후확률을 구하는 것이 목적임. p(x|c) 는 특정그룹 c인 경우에 특정 개체 x가 그룹에 속할 조건부 확률이며, likelihood임. p(c)는 특정 그룹 c가 발생할 빈도, 즉 클래스 사전 고유 확률(prior probability)라고 함.

사후확률 ~ Likelihood x 사전확률 로 요약표현가능

나이브베이지안은

- multi-class 분류에서 쉽고 빠르게 예측이 가능함

- 설명변수들이 모두 독립이라는 가정이 만족한다면, 여타 다른 분류기들(로지스틱,,) 에 비해 훨씬 성능이 좋으며 학습데이터도 적게 필요함.

- 수치형(numeric) 데이터들보단 범주형(categorical) 데이터에 특히 효과적임.

- computation cost가 적기 때문에 매우 빠름.

- train 데이터에는 존재하지 않고 test 데이터에만 존재하는 범주에 대해서는 확률이 0이 되기 때문에 정상적인 예측이 불가능함.

  -> 이것을 zero frequency 라 부르며, 이를 해결하기 위해선 라플라스 추정을 이용하여 smoothing 을 하여야함.

- 설명변수 독립 가정이 성립하지 않을시에는 에러 발생 가능성이 매우 높아져서 결과를 신뢰할 수 없음.