## 빅데이터분석 기출 기반 정리자료01

### 단답형

1. VC Dimension / Hoeffding's Inequality

   vc dimension: 통계적 분류 알고리즘으로 학습 할 수 있는 함수 공간의 용량 -> 모델의 복잡도 라고 이해

   Hoeffding's Inequality(호에프딩 부등식) : 경계(boundary) 의 독립 확률 변수의 합이 예상 값에서 일정량 이상 벗어날 가능성에 대한 상한 값을 제공

   in-sample error와 out-sample error로 내가 가지고 있는 분류기가 있을 때, 학습에 사용된 데이터에 적용시켰을 때 결과와 새로운 데이터에 적용시켰을 때의 결과 차이의 평균을 "확률적"으로 bound 시키는 것

   https://enginius.tistory.com/465

   http://theyearlyprophet.com/learning-from-data-learning-theory-kr.pdf

2. word2vec

   중심단어를 사용하여 주변단어를 예측하는 skip gram 방식과, 주변단어로 중심단어를 예측하는 CBOW 방식이 존재함.

   skip-gram은 지정된 windows에 따라 인풋값들로 중심단어들이 들어가면, 주변단어들을 뉴럴넷을 사용하여 예측하는것임. 우선 중심단어로 원핫벡터 매트릭스를 만든 다음, 임베딩된 단어 벡터를 추출해 내고 벡터에 대한 score 값을 생성해냄. 그 이후 softmax 함수를 사용하여 백터 스코어를 확률값으로 변환해 주변 되는것임.

   여기서 궁금한게 도대체 임베딩된 단어 벡터를 어떻게 추출해 내느냐? 즉 어떻게 단어를 저차원 공간상에 뿌려서 벡터화 하느냐인데… 이를 알기 위해선 word embedding 에 대한 가정을 먼저 알야함. 워드 임베딩에서는 기본적으로 비슷한 분포를 가진 단어들은 비슷한 의미를 지니고 있다고 가정을 함. 쉽게 말해서 단어가 함께 자주 등장하면, 그 단어들은 유사한 단어라고 가정한다는거임. 즉 딥러닝이랑 ai라는게 함께 자주 등장하면 컴퓨터는 이 두가지 단어를 “유사” 하다고 판단하게 되는 것 임. 학습되는 단어의 수가 적으면 힘들겠지만, 충분히 단어의 수가 많다면 함께 자주 등장하는 단어들 역시 많아지게 되고 컴퓨터가 이를 잘 학습할 수 있게 됨.

   모델을 평가하기위해서는 목적함수를 생성해야 하는데.. 베이지안 가정을 사용함. Stochastic Gradient Descent 를 사용해서 파라미터들을 업데이트 해나감.

   Word2vec 에서 주목해야할 방법론(?) 이 두개 잇는데 첫번째는 Negative sampling 에 대한 것임. 간단하게 말해서 softmax에 집어넣는 인풋의 수를 확 줄여서 계산시간을 줄이는건데.. 앞에서 word2vec 이 출력층이 내놓는 스코어값에다가 softmax 함수를 적용해서 이를 확률값으로 변환해 주는 것이라고 했잖음? 그리고 이 값들을 정답과 비교해가면서 계속계속 오차 줄이고 파라미터 튜닝하고 그러는건데.. 소프트맥스를 적용하려면 중심단어와 나머지 모든 단어의 내적을 한 다음에 다시 exp 를 취해주어야함.

   p(o|c)=exp(uTovc) / ∑Ww=1*exp(uTwvc) 이 식을 최대화 하는 방향으로 학습이 진행되기 때문임. 식을 간단하게 말하면 중심단어가 주어졌을 때 주변단어가 나타날 확률.. 앞서 말한 베이지안 조건부 확률이 이걸 말하는듯? 아무튼 목적함수는 저거임.

   다시 negative sampling 으로 돌아가서.. 모든 출력값에 대해 소프트맥스를 적용하려면 계산량이 어마어마하다는거임… 그래서 소프트맥스 확률을 구할 때 전체 단어를 대상으로 하지 않고, 일부 단어만 뽑아서 계산을 하는데, 이게 negative sampling 임.

   사용자가 지정한 윈도우 사이즈 내에 등장하지 않은 단어, 즉 네거티브 샘플을 5-20개 정도 뽑고 이를 정답단어와 합쳐 전체 단어처럼 소프트맥스 확률을 구하는 거임. 이렇게 되면 계산량을 획기적으로 줄이는 것이 가능해짐.

   두번째는 hierarchical softmax 인데.. 애는 전체 단어를 표현하는데에 binary 한 tree를 활용함. 각 트리의 리프가 단어고 트리 각각의 노드는 모델이 학습할 vector 와 연관이 되어 있음. 모든 단어들에 대한 확률을 계산하기 위해 트리구조를 활용하여 목적함수를 정의한다고 보면 됨.

   각 단어들을 leaf로 가지는 binary tree를 하나 만들어 놓든 다음 해당하는 단어의 확률을 계산할 때 root 에서부터 해당 leaf로 가는 path 에 따라서 확률을 곱해나가는 식으로 해당 단어가 발생할 최종적인 확률을 계산하는것임.. 이 방법역시 일반적인 소프트맥스 방법에 비해 계산량을 줄여주는데 binary tree를 잘 만들었을 경우 평균적으로 루트로부터 leaf까지의 거리는 평균 O(lnV)일 것이므로 총 N x lnV의 계산량만으로 특정 단어가 나올 확률을 계산할 수 있음

3. Glove

   LSA(Latent semantic analysis)는 말뭉치 전체의 통계적인 정보를 모두 활용하지만, LSA 결과물을 가지고 단어/문서 간 유사도를 측정하기는 어려운 단점을 지님. 반대로 Word2Vec(Skip-Gram)은 저차원 벡터공간에 임베딩된 단어벡터 사이의 유사도를 측정하는 데는 LSA에 비해 좋은 성능을 가지지만, 사용자가 지정한 윈도우(주변 단어 몇개만 볼지) 내에서만 학습/분석이 이뤄지기 때문에 말뭉치 전체의 공기정보(co-occurrence)는 반영되기 어려운 단점을 지니고 있음.

   이 때문에 GloVe 연구팀은 임베딩된 두 단어벡터의 내적이 말뭉치 전체에서의 동시 등장확률 로그값이 되도록 목적함수를 정의함. (their dot product equals the logarithm of the words’ probability of co-occurrence) 간단히 말해, “임베딩된 단어벡터 간 유사도 측정을 수월하게 하면서도 말뭉치 전체의 통계 정보를 좀 더 잘 반영해보자”가 GloVe가 지향하는 핵심 목표임

   Glove는 특정 단어 k 가 주어졌을 때 임베딩 된 두 단어벡터의 내적값이 두 단어의 동시등장확률 간 비율이 되게끔 임베딩 하려 했음. 여기서 동시등장확률이란, 예를들어 아이스크림/달콤함 이 두 단어가 동시에 등장할 확률이 아이스크림/주유소 보다 확연히 높은데 이를 반영한다고 생각하면 됨.

   이렇게 단어간 동시등장비율 정보를 한꺼번에 반영하면 좀 더 정확한 단어 임베딩이 될 거라는게 연구진들의 생각이었음.

   단어의 co-occurrence 매트릭스에서 0이 아닌 요소들을 모두 활용하여 모델 학습을 진행하고 vecter space를 생성하기 때문에 word2vec 보다 성능이 좋음(word2vec 는 단어사이의 동시등장확률 정보는 고려하지 않음. 그러나 글로브는 고려함. 또한 word2vec 는 사용자가 지정한 윈도우에 대한 것들만 인풋으로 들어감. 글로브는 동시등장확률을 고려함으로 인해 단어 전체에 대한 정보를 인풋값으로 활용한다고 볼 수 있음)

4. Q-Q plot

   수집 데이터를 표준정규분포의 분위수와 비교하여 그리는 그래프로, 수집 데이터가 정규성 가정을 만족할시 y = x 형태의 직선 모양으로 그려짐.

5. 시계열 분석 검증법

   시계열 분석..

   - Arima 모형 (승법계절 아리마 p,d,q 모형)

     데이터가 정상 시계열인지 확인

     -> 해당 시계열 데이터가 시간에 독립인지 / 시계열의 분산이 일정한지 / 시계열의 평균 M 이 시간에 대해 constant 한지 확인 (다 같은말임)

     정상 시계열이 아니라면 (데이터에 추세가 보인다면) 적절하게 차분을 시행하여 시계열을 정상시계열로 변환해주는데, 비계절 차분과 계절차분으로 나뉘며 계절성이 뚜렷하게 보이는 비정상 시계열에는 계절 차분을 시행하고, 그렇지않다면 비계절 차분을 시행함.

     **이후 분석하는 시계열 데이터는 정상성 가정을 만족하며, 잔차는 자가상관을 가지고 있지 않은 백색잡음이라고 가정함**

   - 검정방법

     1. 단위근 검정 -> 차분을 진행 할지 말지 결정

        ∇Yt=aYt−1+ϕ1∇Yt−1+ϕ2∇Yt−2+⋯+ϕp∇Yt−p+et ( yt의 변화량을 종속변수, 원 시계열과 차분값의 지연 값들을 독립변수로 회귀분석을 시행하여 구한 계수값 a)

        H0: 데이터가 정상 시계열을 만족한다 (a = 0)

        H1: 데이터가 정상 시계열을 만족하지 않는다.(a !=0)

        피벨류가 유의확률보다 작으면(검정통계량 값이 유의수준의 Z값보다 크면) 귀무가설 기각 -> 데이터가 정상성 가정을 만족하지 않으므로 데이터 차분을 진행하여야 함.

     2. 포트만토 검정 -> 잔차의 자가상관을 확인함

        H0: 잔차가 자가상관이 없음(p1=p2...=0)

        H1: 잔차에 자가상관이 존재함

        피벨류가 유의확률보다 커 귀무가설이 채택되면 -> 잔차가 자가상관이 없으므로 모형이 제대로 만들어진 것.

        피벨류가 유의확률보다 작아 귀무가설이 기각되면 -> 잔차에 자가상관이 남아있는 것이므로 모형 수정이 필요함.

6. 가설검정/기초통계

   - 가설검정

     -> 시계열 검정 부분에서 먼저 나왔는데, 통계의 기본 가설검정은 귀무가설, 대립가설 세우는것부터 시작

     귀무가설 : m =0 이다의 형태 (독립이다/ 계수가 유의미하지않다 )

     대립가설 : m>0 or m<0 (단측검정) , m != 0 (양측검정)

     단측검정의 경우 검정통계량 값 > Z값 (m>0) 검정통계량 값 < Z값( m<0 ) 이면 귀무가설 기각, 양측검정은 검정통계량 > |Z/2| 면 귀무가설 기각.

   - 기초통계

     https://blog.naver.com/PostView.nhn?blogId=mykepzzang&logNo=220842759639&parentCategoryNo=&categoryNo=&viewDate=&isShowPopularPosts=false&from=postView

     이항분포(binomial): 확률 p 인 사건을 N번 시행하여 사건 발행 횟수에 따른 확률들을 구하면 그것을 확률 p, 시행횟수 N인 이항분포라고 정의하며 B(N,p)로 표현함. 평균은 Np,이며 분산은 Np*(1-p) 임 중심극한정리에 따라 N 이 커지면서 정규분포로 근사함

     포아송분포(poisson): 단위시간 안에 어떤 사건이 몇 번 발생할 것인지 표현하는 이산 확률 분포로, 일어나는 사건이 독립적임. 평균과 분산 모두 모수 람다 값이며 np의 값이 5보다 작으면 이항분포는 람다= np 인 포아송 분포에 근사하게 됨 (주로 count 데이터에 활용함)

     초기하분포(hypergeometric): 크기가 N인 유한한 모집단으로부터 n개를 비복원추출 할 시에 확률변수 X가 나타내는 이산 확률분포로, 평균은 nk/N이며 분산은 (N-n/N-1)n(k/N)*(1-k/N) 임

     기하분포(geometric): 성공일 확률이 p 인 베르누이 시행을 독립적으로 반복하고 처음 성공할 때까지의 시행횟수를 확률변수 X라 할때, X가 나타내는 이산확률분포. 평균은 1/p 이며 분산은 1-p/p^2 . 음이항 분포에서 k =1 인 특수한 경우임.

     감마분포(Gamma): 알파 번째 사건이 일어 날 때까지 걸리는 시간에 대한 연속 확률분포, 평균은 알파와 베타의 곱이며 변수는 알파와 베타 제곱의 곱임

     지수분포(exponential): 사건이 서로 독립적일때, 일정시간 동안 발생하는 사건의 횟수가 포아송 분포를 따른다면 다음 사건이 일어날 때까지의 시간에 대한 분포는 지수분포를 따름 감마분포에서 알파값을 1로 고정시킨 특수한 경우이며 평균은 모수 베타값이며 분산은 베타제곱임

     음이항분포(Negative Binomial): 성공확률이 p 인 베르누이 시행을 독립적으로 반복하고, k번째 성공이 나올때까지의 시행횟수를 확률변수 X라 할때, X가 나타내는 이산확률분포 평균은 k/p, 분산은 k(1-p)/p^2

     카이제곱분포(chi-square): 감마분포에서 알파가 v/2 베타가 2에 해당하는 특수한 경우로 자유도 k 에 따라 분포가 달라짐. 연속확률변수 X가 평균이 m 이고 분산이 sigma^2 인 정규분포를 따를때, (x-m)^2/sigma^2 은 자유도 1인 카이제곱 분포를 따름.

     T분포: 모집단의 분산이 알려져 있지 않은 경우에 정규분포 대신 이용하는 확률분포로, 평균이 m이고 분산이 sigma^2인 정규분포에서 추출한 표본 x1,x2..xn에 대한 표본분산을 S^2라고 할때 T = mean(x)-m/(S/n^0.5) 는 자유도가 n-1 인 티분포를 따름

     F분포:정규분포를 이루는 모집단에서 독립적으로 추출한 표본들의 분산 비가 나타내는 연속 확률 분포로, 2개 이상의 표본평균들이 동일한 모평균을 가진 집단에서 추출되었는지 아닌지를 판단하기 위해 이용함. 서로 독립인 확률변수 U,V가 각각 자유도가 u,v인 카이제곱분포를 따를때 새 확률변수 F = (U/u)/(V/v) 는 자유도가 (u,v) 인 F분포를 따름